{"stageStartTimestamp":1667886615211527072,"stageEndTimestamp":1667886635776959351,"tasks":[{"container":{"reused":true,"requestId":"7e104a68-7fde-4dde-b740-3ce3c88e8145","uuid":"26700c8c-c0db-4648-9e9d-d1e518e0ca84","msRemaining":590846,"requestsServed":3,"startTimestamp":1667885960965808763,"deadlineTimestamp":1667887215317000000},"invoked_containers":[],"log":"[2022-11-08 05:49:48.324] [Lambda worker] [info] performing global initialization (Worker App)\n[2022-11-08 05:49:48.338] [Lambda worker] [info] Invoking WorkerApp fallback\n[2022-11-08 05:49:48.351] [Lambda worker] [info] Found 1 input URIs to process\n[2022-11-08 05:49:48.351] [Lambda worker] [info] *** hyperspecialization active ***\n[2022-11-08 05:49:48.351] [Lambda worker] [info] -- specializing to s3://tuplex-public/data/github_daily/2011-10-15.json:0-78478920\n[2022-11-08 05:49:48.351] [hyper specializer] [info] specializing code to file s3://tuplex-public/data/github_daily/2011-10-15.json:0-78478920\n[2022-11-08 05:49:48.352] [hyper specializer] [info] Decompressed Code context from 2.16 KB to 47.96 KB\n[2022-11-08 05:49:48.380] [codegen] [warning] could not find column 'repository' in dataset. Emitting deoptimization trigger.\n[2022-11-08 05:49:48.381] [global] [info] retrieving pythonic sample took: 0.000266s\n[2022-11-08 05:49:48.417] [codegen] [warning] could not find column 'repository' in dataset. Emitting deoptimization trigger.\n[2022-11-08 05:49:48.484] [global] [info] sampled s3://tuplex-public/data/github_daily/2011-10-15.json on 256.00 KB\n[2022-11-08 05:49:48.484] [fileinputoperator] [info] Parallel sample fetch done.\n[2022-11-08 05:49:48.484] [fileinputoperator] [info] Filling sample cache for json operator took 0.065763s (1 entries, 0 rows)\n[2022-11-08 05:49:48.526] [global] [info] sampled s3://tuplex-public/data/github_daily/2011-10-15.json on 256.00 KB\n[2022-11-08 05:49:48.696] [global] [info] sampled s3://tuplex-public/data/github_daily/2011-10-15.json on 256.00 KB\n[2022-11-08 05:49:48.901] [fileinputoperator] [info] Parallel sample parse done.\n[2022-11-08 05:49:48.901] [fileinputoperator] [info] Extracting row sample took 0.416712s\n[2022-11-08 05:49:48.948] [global] [info] retrieving pythonic sample took: 0.529705s\n[2022-11-08 05:49:48.958] [hyper specializer] [info] Deserialization of Code context took 0.606132s\n[2022-11-08 05:49:48.958] [hyper specializer] [info] Total Stage Decode took 0.606520s\n[2022-11-08 05:49:49.013] [global] [info] sampled s3://tuplex-public/data/github_daily/2011-10-15.json:0-78478920 on 256.00 KB\n[2022-11-08 05:49:49.013] [fileinputoperator] [info] Parallel sample fetch done.\n[2022-11-08 05:49:49.014] [fileinputoperator] [info] Filling sample cache for json operator took 0.027990s (1 entries, 0 rows)\n[2022-11-08 05:49:49.043] [global] [info] sampled s3://tuplex-public/data/github_daily/2011-10-15.json:0-78478920 on 256.00 KB\n[2022-11-08 05:49:49.142] [global] [info] sampled s3://tuplex-public/data/github_daily/2011-10-15.json:0-78478920 on 256.00 KB\n[2022-11-08 05:49:49.276] [fileinputoperator] [info] Parallel sample parse done.\n[2022-11-08 05:49:49.276] [fileinputoperator] [info] Extracting row sample took 0.261991s\n[2022-11-08 05:49:49.443] [hyper specializer] [info] sampling (setInputFiles) took 0.485180s\n[2022-11-08 05:49:49.443] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.443] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.443] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.443] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.443] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.443] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.443] [global] [warning] requested 2000 rows for sampling, but only 272 stored. Consider decreasing sample size. Returning all available rows.\n[2022-11-08 05:49:49.454] [specializing stage optimizer] [info] Row type before retype: (str,boolean,Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)],str,Struct[(str,'commits'->List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'head'->str),(str,'legacy'->Struct[(str,'head'->str),(str,'size'->i64),(str,'push_id'->i64),(str,'shas'->List[List[str]]),(str,'ref'->str)]),(str,'push_id'->i64),(str,'ref'->str),(str,'size'->i64)],str,Struct[(str,'url'->str),(str,'id'->i64),(str,'name'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]])\n[2022-11-08 05:49:49.454] [specializing stage optimizer] [info] Row type after retype: (str,boolean,Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)],str,Struct[(str,'head'->str),(str,'size'->i64),(str,'push_id'->i64),(str,'ref'->str),(str,'commits'->List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'legacy'->Struct[(str,'head'->str),(str,'size'->i64),(str,'push_id'->i64),(str,'shas'->List[List[str]]),(str,'ref'->str)])],str,Struct[(str,'url'->str),(str,'id'->i64),(str,'name'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]])\n[2022-11-08 05:49:49.500] [codegen] [warning] could not find column 'repository' in dataset. Emitting deoptimization trigger.\n[2022-11-08 05:49:49.644] [hyper specializer] [info] specialized to input:  (str,boolean,Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)],str,Struct[(str,'commits'->List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'head'->str),(str,'legacy'->Struct[(str,'head'->str),(str,'size'->i64),(str,'push_id'->i64),(str,'shas'->List[List[str]]),(str,'ref'->str)]),(str,'push_id'->i64),(str,'ref'->str),(str,'size'->i64)],str,Struct[(str,'url'->str),(str,'id'->i64),(str,'name'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]])\n[2022-11-08 05:49:49.644] [hyper specializer] [info] specialized to output: (str)\n[2022-11-08 05:49:49.644] [hyper specializer] [info] specialized code reads: 8 columns\n[2022-11-08 05:49:49.644] [codegen] [info] module's data layout is:\n[2022-11-08 05:49:49.647] [codegen] [info] generating pipeline for (str,boolean,Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)],str,Struct[(str,'commits'->List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'head'->str),(str,'legacy'->Struct[(str,'head'->str),(str,'size'->i64),(str,'push_id'->i64),(str,'shas'->List[List[str]]),(str,'ref'->str)]),(str,'push_id'->i64),(str,'ref'->str),(str,'size'->i64)],str,Struct[(str,'url'->str),(str,'id'->i64),(str,'name'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]]) -> (str) (5 operators pipelined)\n[2022-11-08 05:49:49.650] [codegen] [info] generating lambda function for ((str,boolean,Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)],str,Struct[(str,'commits'->List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'head'->str),(str,'legacy'->Struct[(str,'head'->str),(str,'size'->i64),(str,'push_id'->i64),(str,'shas'->List[List[str]]),(str,'ref'->str)]),(str,'push_id'->i64),(str,'ref'->str),(str,'size'->i64)],str,Struct[(str,'url'->str),(str,'id'->i64),(str,'name'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]])) -> i64\n[2022-11-08 05:49:49.653] [codegen] [info] generating function extract_repo_id_code for ((str,boolean,Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)],str,Struct[(str,'commits'->List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'head'->str),(str,'legacy'->Struct[(str,'head'->str),(str,'size'->i64),(str,'push_id'->i64),(str,'shas'->List[List[str]]),(str,'ref'->str)]),(str,'push_id'->i64),(str,'ref'->str),(str,'size'->i64)],str,Struct[(str,'url'->str),(str,'id'->i64),(str,'name'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]],i64)) -> i64\n[2022-11-08 05:49:49.655] [codegen] [info] generating lambda function for ((str,boolean,Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)],str,Struct[(str,'commits'->List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'head'->str),(str,'legacy'->Struct[(str,'head'->str),(str,'size'->i64),(str,'push_id'->i64),(str,'shas'->List[List[str]]),(str,'ref'->str)]),(str,'push_id'->i64),(str,'ref'->str),(str,'size'->i64)],str,Struct[(str,'url'->str),(str,'id'->i64),(str,'name'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]],i64,i64)) -> boolean\n[2022-11-08 05:49:49.656] [codegen] [info] generating lambda function for ((str,boolean,Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)],str,Struct[(str,'commits'->List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'head'->str),(str,'legacy'->Struct[(str,'head'->str),(str,'size'->i64),(str,'push_id'->i64),(str,'shas'->List[List[str]]),(str,'ref'->str)]),(str,'push_id'->i64),(str,'ref'->str),(str,'size'->i64)],str,Struct[(str,'url'->str),(str,'id'->i64),(str,'name'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]],i64,i64)) -> (str,i64,i64)\n[2022-11-08 05:49:49.658] [codegen] [warning] hack, need to fix stuff here...\n[2022-11-08 05:49:49.665] [hyper specializer] [info] generated code in 0.021532s\n[2022-11-08 05:49:49.707] [Lambda worker] [info] specialized normal-case type (str,boolean,Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)],str,Struct[(str,'commits'->List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'head'->str),(str,'legacy'->Struct[(str,'head'->str),(str,'size'->i64),(str,'push_id'->i64),(str,'shas'->List[List[str]]),(str,'ref'->str)]),(str,'push_id'->i64),(str,'ref'->str),(str,'size'->i64)],str,Struct[(str,'url'->str),(str,'id'->i64),(str,'name'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]]) is different than given normal-case type (str,boolean,Struct[(str,'avatar_url'->str),(str,'gravatar_id'=>str),(str,'id'=>i64),(str,'login'=>str),(str,'url'->str)],str,Struct[(str,'action'=>str),(str,'comment'=>Struct[(str,'body'->str),(str,'commit_id'=>str),(str,'created_at'->str),(str,'id'->i64),(str,'line'=>null),(str,'path'=>null),(str,'position'=>null),(str,'updated_at'->str),(str,'url'->str),(str,'user'->Struct[(str,'avatar_url'->str),(str,'gravatar_id'->str),(str,'id'->i64),(str,'login'->str),(str,'url'->str)])]),(str,'commits'=>List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'description'=>str),(str,'forkee'=>Struct[(str,'clone_url'->str),(str,'created_at'->str),(str,'description'->str),(str,'fork'->boolean),(str,'forks'->i64),(str,'git_url'->str),(str,'homepage'->str),(str,'html_url'->str),(str,'id'->i64),(str,'language'->Option[str]),(str,'master_branch'->null),(str,'name'->str),(str,'open_issues'->i64),(str,'owner'->Struct[(str,'avatar_url'->str),(str,'gravatar_id'->str),(str,'url'->str),(str,'id'->i64),(str,'login'->str)]),(str,'private'->boolean),(str,'public'->boolean),(str,'pushed_at'->str),(str,'size'->i64),(str,'ssh_url'->str),(str,'svn_url'->str),(str,'updated_at'->str),(str,'url'->str),(str,'watchers'->i64)]),(str,'gist'=>Struct[(str,'comments'->i64),(str,'created_at'->str),(str,'description'->str),(str,'files'->{}),(str,'git_pull_url'->str),(str,'git_push_url'->str),(str,'html_url'->str),(str,'id'->str),(str,'public'->boolean),(str,'updated_at'->str),(str,'url'->str),(str,'user'->Option[Struct[(str,'avatar_url'->str),(str,'gravatar_id'->str),(str,'url'->str),(str,'id'->i64),(str,'login'->str)]])]),(str,'head'=>str),(str,'issue'=>Struct[(str,'assignee'->Option[Struct[(str,'gravatar_id'->str),(str,'avatar_url'->str),(str,'url'->str),(str,'id'->i64),(str,'login'->str)]]),(str,'body'->str),(str,'closed_at'->Option[str]),(str,'comments'->i64),(str,'created_at'->str),(str,'html_url'->str),(str,'id'->i64),(str,'labels'->List[Struct[(str,'name'->str),(str,'url'->str),(str,'color'->str)]]),(str,'milestone'->Option[Struct[(str,'number'->i64),(str,'created_at'->str),(str,'due_on'->str),(str,'title'->str),(str,'creator'->Struct[(str,'gravatar_id'->str),(str,'avatar_url'->str),(str,'url'->str),(str,'id'->i64),(str,'login'->str)]),(str,'url'->str),(str,'open_issues'->i64),(str,'closed_issues'->i64),(str,'description'->str),(str,'state'->str)]]),(str,'number'->i64),(str,'pull_request'->Struct[(str,'diff_url'->Option[str]),(str,'html_url'->Option[str]),(str,'patch_url'->Option[str])]),(str,'state'->str),(str,'title'->str),(str,'updated_at'->str),(str,'url'->str),(str,'user'->Struct[(str,'avatar_url'->str),(str,'gravatar_id'->str),(str,'id'->i64),(str,'login'->str),(str,'url'->str)])]),(str,'legacy'=>Struct[(str,'action'=>str),(str,'comment_id'=>i64),(str,'commit'=>str),(str,'desc'=>str),(str,'head'=>str),(str,'id'=>i64),(str,'issue'=>i64),(str,'issue_id'=>i64),(str,'name'=>str),(str,'number'=>i64),(str,'push_id'=>i64),(str,'ref'=>str),(str,'shas'=>List[List[str]]),(str,'size'=>i64),(str,'url'=>str)]),(str,'master_branch'=>str),(str,'pages'=>List[Struct[(str,'sha'->str),(str,'title'->str),(str,'action'->str),(str,'page_name'->str),(str,'summary'->null)]]),(str,'push_id'=>i64),(str,'ref'=>Option[str]),(str,'ref_type'=>str),(str,'size'=>i64)],str,Struct[(str,'id'=>i64),(str,'name'->str),(str,'url'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]]).\n[2022-11-08 05:49:49.707] [Lambda worker] [info] -- hyperspecialization took 1.355879s\n[2022-11-08 05:49:49.748] [global] [info] TransformStage - Optimization via LLVM passes took 0.040620 ms\n[2022-11-08 05:49:49.748] [global] [info] syms registered (or skipped), compile now\n[2022-11-08 05:49:49.792] [global] [info] functor fast_Stage_0 retrieved from llvm\n[2022-11-08 05:49:49.792] [global] [info] Compiled code paths for stage 0 in 0.08 ms\n[2022-11-08 05:49:49.792] [Lambda worker] [info] calling initStageFunctor with 0 args\n[2022-11-08 05:49:51.566] [Lambda worker] [info] fast path took: 1.773551s\n[2022-11-08 05:49:51.566] [Lambda worker] [info] Input rows processed: normal: 3526 unresolved: 45372\n[2022-11-08 05:49:51.566] [Lambda worker] [info] Normal rows before resolve: 0, except rows before resolve: 45372\n[2022-11-08 05:49:51.566] [Lambda worker] [info] Starting exception resolution/slow path execution\n[2022-11-08 05:49:51.566] [Lambda worker] [info] has interpreter path: yes has compiled fallback path: yes\n[2022-11-08 05:49:51.566] [Lambda worker] [info] compiling slow code path b.c. exceptions occurred.\n[2022-11-08 05:49:52.978] [Lambda worker] [info] Compilation of slow path took 1.412470s\n[2022-11-08 05:49:52.992] [Lambda worker] [info] first unresolved row ec code result is: ec code: 112\n[2022-11-08 05:49:55.119] [Lambda worker] [info] Resolved buffer, compiled: 2477 interpreted: 19358 unresolved: 614\n[2022-11-08 05:49:55.148] [Lambda worker] [info] first unresolved row ec code result is: ec code: 112\n[2022-11-08 05:49:57.391] [Lambda worker] [info] Resolved buffer, compiled: 2765 interpreted: 19488 unresolved: 670\n[2022-11-08 05:49:57.393] [Lambda worker] [info] Exception resolution/slow path done. Took 5.826915s\n[2022-11-08 05:49:57.393] [Lambda worker] [info] Data processed in 7.60063s 31.13 KB (1418 normal rows) 1.52 MB (1284 exception rows)  128.03 KB (0 hash rows)\n[2022-11-08 05:49:57.393] [Lambda worker] [info] Writing data to s3://tuplex-leonhard/experiments/flights_github/hyper/part0.csv.csv\n[2022-11-08 05:49:57.393] [Lambda worker] [info] file output initiated...\n[2022-11-08 05:49:57.393] [Lambda worker] [info] Writing output from 2 parts (31.13 KB, 1418 rows)\n[2022-11-08 05:49:57.393] [Lambda worker] [info] Merging output parts together into s3://tuplex-leonhard/experiments/flights_github/hyper/part0.csv.csv\n[2022-11-08 05:49:57.393] [s3fs] [info] Writing buffer of size 18\n[2022-11-08 05:49:57.393] [s3fs] [info] Writing buffer of size 16876\n[2022-11-08 05:49:57.393] [s3fs] [info] Writing buffer of size 15000\n[2022-11-08 05:49:57.393] [s3fs] [info] Invoking lazyUpload to uri s3://tuplex-leonhard/experiments/flights_github/hyper/part0.csv.csv\n[2022-11-08 05:49:57.393] [s3fs] [info] Issuing simple write request\n[2022-11-08 05:49:57.470] [Lambda worker] [info] file output done.\n[2022-11-08 05:49:57.470] [Lambda worker] [info] Data fully materialized\n[2022-11-08 05:49:57.470] [Lambda worker] [info] Took 7.678486s in total\n[2022-11-08 05:49:57.470] [Lambda worker] [info] Paths rows took: normal: 3526 general: 5242 interpreter: 38846 unresolved: 1284\n","invoked_requests":[],"input_uris":["s3://tuplex-public/data/github_daily/2011-10-15.json:0-78478920"]},{"container":{"reused":true,"requestId":"ef326473-8f92-4979-9e97-6177434ea0ed","uuid":"d9c04bb4-f3f8-43d3-92ec-af2fd7adc394","msRemaining":579690,"requestsServed":3,"startTimestamp":1667885960807303977,"deadlineTimestamp":1667887215373000000},"invoked_containers":[],"log":"[2022-11-08 05:49:48.380] [Lambda worker] [info] performing global initialization (Worker App)\n[2022-11-08 05:49:48.394] [Lambda worker] [info] Invoking WorkerApp fallback\n[2022-11-08 05:49:48.407] [Lambda worker] [info] Found 1 input URIs to process\n[2022-11-08 05:49:48.407] [Lambda worker] [info] *** hyperspecialization active ***\n[2022-11-08 05:49:48.407] [Lambda worker] [info] -- specializing to s3://tuplex-public/data/github_daily/2013-10-15.json:0-268435456\n[2022-11-08 05:49:48.407] [hyper specializer] [info] specializing code to file s3://tuplex-public/data/github_daily/2013-10-15.json:0-268435456\n[2022-11-08 05:49:48.408] [hyper specializer] [info] Decompressed Code context from 2.16 KB to 47.96 KB\n[2022-11-08 05:49:48.436] [codegen] [warning] could not find column 'repository' in dataset. Emitting deoptimization trigger.\n[2022-11-08 05:49:48.437] [global] [info] retrieving pythonic sample took: 0.000274s\n[2022-11-08 05:49:48.474] [codegen] [warning] could not find column 'repository' in dataset. Emitting deoptimization trigger.\n[2022-11-08 05:49:48.475] [global] [info] retrieving pythonic sample took: 0.000272s\n[2022-11-08 05:49:48.485] [hyper specializer] [info] Deserialization of Code context took 0.077106s\n[2022-11-08 05:49:48.485] [hyper specializer] [info] Total Stage Decode took 0.077490s\n[2022-11-08 05:49:48.600] [global] [info] sampled s3://tuplex-public/data/github_daily/2013-10-15.json:0-268435456 on 256.00 KB\n[2022-11-08 05:49:48.600] [fileinputoperator] [info] Parallel sample fetch done.\n[2022-11-08 05:49:48.600] [fileinputoperator] [info] Filling sample cache for json operator took 0.115520s (1 entries, 0 rows)\n[2022-11-08 05:49:48.661] [global] [info] sampled s3://tuplex-public/data/github_daily/2013-10-15.json:0-268435456 on 256.00 KB\n[2022-11-08 05:49:48.827] [global] [info] sampled s3://tuplex-public/data/github_daily/2013-10-15.json:0-268435456 on 256.00 KB\n[2022-11-08 05:49:49.074] [fileinputoperator] [info] Parallel sample parse done.\n[2022-11-08 05:49:49.074] [fileinputoperator] [info] Extracting row sample took 0.474029s\n[2022-11-08 05:49:49.354] [hyper specializer] [info] sampling (setInputFiles) took 0.868929s\n[2022-11-08 05:49:49.354] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.354] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.354] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.354] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.354] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.354] [global] [error] fatal error: TupleTree can be only constructed using nested tuples so far! Given type is Exception\n[2022-11-08 05:49:49.354] [global] [warning] requested 2000 rows for sampling, but only 290 stored. Consider decreasing sample size. Returning all available rows.\n[2022-11-08 05:49:49.364] [specializing stage optimizer] [info] Row type before retype: (str,Struct[(str,'shas'->List[(str,str,str,str,boolean)]),(str,'size'->i64),(str,'ref'->str),(str,'head'->str)],boolean,str,str,str,Struct[(str,'login'->str),(str,'type'->str),(str,'gravatar_id'->str),(str,'email'->str)],Struct[(str,'id'->i64),(str,'name'->str),(str,'url'->str),(str,'description'->str),(str,'watchers'->i64),(str,'stargazers'->i64),(str,'forks'->i64),(str,'fork'->boolean),(str,'size'->i64),(str,'owner'->str),(str,'private'->boolean),(str,'open_issues'->i64),(str,'has_issues'->boolean),(str,'has_downloads'->boolean),(str,'has_wiki'->boolean),(str,'language'->str),(str,'created_at'->str),(str,'pushed_at'->str),(str,'master_branch'->str)])\n[2022-11-08 05:49:49.364] [specializing stage optimizer] [info] Row type after retype: (str,Struct[(str,'shas'->List[(str,str,str,str,boolean)]),(str,'size'->i64),(str,'ref'->str),(str,'head'->str)],boolean,str,str,str,Option[Struct[(str,'login'->str),(str,'type'->str),(str,'gravatar_id'->str),(str,'email'->str)]],Option[Struct[(str,'id'->i64),(str,'name'->str),(str,'url'->str),(str,'description'->str),(str,'watchers'->i64),(str,'stargazers'->i64),(str,'forks'->i64),(str,'fork'->boolean),(str,'size'->i64),(str,'owner'->str),(str,'private'->boolean),(str,'open_issues'->i64),(str,'has_issues'->boolean),(str,'has_downloads'->boolean),(str,'has_wiki'->boolean),(str,'language'->str),(str,'created_at'->str),(str,'pushed_at'->str),(str,'master_branch'->str)]])\n[2022-11-08 05:49:49.436] [codegen] [warning] could not find column 'repo' in dataset. Emitting deoptimization trigger.\n[2022-11-08 05:49:49.546] [hyper specializer] [info] specialized to input:  (str,Struct[(str,'shas'->List[(str,str,str,str,boolean)]),(str,'size'->i64),(str,'ref'->str),(str,'head'->str)],boolean,str,str,str,Struct[(str,'login'->str),(str,'type'->str),(str,'gravatar_id'->str),(str,'email'->str)],Struct[(str,'id'->i64),(str,'name'->str),(str,'url'->str),(str,'description'->str),(str,'watchers'->i64),(str,'stargazers'->i64),(str,'forks'->i64),(str,'fork'->boolean),(str,'size'->i64),(str,'owner'->str),(str,'private'->boolean),(str,'open_issues'->i64),(str,'has_issues'->boolean),(str,'has_downloads'->boolean),(str,'has_wiki'->boolean),(str,'language'->str),(str,'created_at'->str),(str,'pushed_at'->str),(str,'master_branch'->str)])\n[2022-11-08 05:49:49.546] [hyper specializer] [info] specialized to output: (str)\n[2022-11-08 05:49:49.546] [hyper specializer] [info] specialized code reads: 8 columns\n[2022-11-08 05:49:49.546] [codegen] [info] module's data layout is:\n[2022-11-08 05:49:49.549] [codegen] [info] generating pipeline for (str,Struct[(str,'shas'->List[(str,str,str,str,boolean)]),(str,'size'->i64),(str,'ref'->str),(str,'head'->str)],boolean,str,str,str,Struct[(str,'login'->str),(str,'type'->str),(str,'gravatar_id'->str),(str,'email'->str)],Struct[(str,'id'->i64),(str,'name'->str),(str,'url'->str),(str,'description'->str),(str,'watchers'->i64),(str,'stargazers'->i64),(str,'forks'->i64),(str,'fork'->boolean),(str,'size'->i64),(str,'owner'->str),(str,'private'->boolean),(str,'open_issues'->i64),(str,'has_issues'->boolean),(str,'has_downloads'->boolean),(str,'has_wiki'->boolean),(str,'language'->str),(str,'created_at'->str),(str,'pushed_at'->str),(str,'master_branch'->str)]) -> (str) (5 operators pipelined)\n[2022-11-08 05:49:49.552] [codegen] [info] generating lambda function for ((str,Struct[(str,'shas'->List[(str,str,str,str,boolean)]),(str,'size'->i64),(str,'ref'->str),(str,'head'->str)],boolean,str,str,str,Struct[(str,'login'->str),(str,'type'->str),(str,'gravatar_id'->str),(str,'email'->str)],Struct[(str,'id'->i64),(str,'name'->str),(str,'url'->str),(str,'description'->str),(str,'watchers'->i64),(str,'stargazers'->i64),(str,'forks'->i64),(str,'fork'->boolean),(str,'size'->i64),(str,'owner'->str),(str,'private'->boolean),(str,'open_issues'->i64),(str,'has_issues'->boolean),(str,'has_downloads'->boolean),(str,'has_wiki'->boolean),(str,'language'->str),(str,'created_at'->str),(str,'pushed_at'->str),(str,'master_branch'->str)])) -> i64\n[2022-11-08 05:49:49.555] [codegen] [info] generating function extract_repo_id_code for ((str,Struct[(str,'shas'->List[(str,str,str,str,boolean)]),(str,'size'->i64),(str,'ref'->str),(str,'head'->str)],boolean,str,str,str,Struct[(str,'login'->str),(str,'type'->str),(str,'gravatar_id'->str),(str,'email'->str)],Struct[(str,'id'->i64),(str,'name'->str),(str,'url'->str),(str,'description'->str),(str,'watchers'->i64),(str,'stargazers'->i64),(str,'forks'->i64),(str,'fork'->boolean),(str,'size'->i64),(str,'owner'->str),(str,'private'->boolean),(str,'open_issues'->i64),(str,'has_issues'->boolean),(str,'has_downloads'->boolean),(str,'has_wiki'->boolean),(str,'language'->str),(str,'created_at'->str),(str,'pushed_at'->str),(str,'master_branch'->str)],i64)) -> i64\n[2022-11-08 05:49:49.557] [codegen] [info] generating lambda function for ((str,Struct[(str,'shas'->List[(str,str,str,str,boolean)]),(str,'size'->i64),(str,'ref'->str),(str,'head'->str)],boolean,str,str,str,Struct[(str,'login'->str),(str,'type'->str),(str,'gravatar_id'->str),(str,'email'->str)],Struct[(str,'id'->i64),(str,'name'->str),(str,'url'->str),(str,'description'->str),(str,'watchers'->i64),(str,'stargazers'->i64),(str,'forks'->i64),(str,'fork'->boolean),(str,'size'->i64),(str,'owner'->str),(str,'private'->boolean),(str,'open_issues'->i64),(str,'has_issues'->boolean),(str,'has_downloads'->boolean),(str,'has_wiki'->boolean),(str,'language'->str),(str,'created_at'->str),(str,'pushed_at'->str),(str,'master_branch'->str)],i64,i64)) -> boolean\n[2022-11-08 05:49:49.559] [codegen] [info] generating lambda function for ((str,Struct[(str,'shas'->List[(str,str,str,str,boolean)]),(str,'size'->i64),(str,'ref'->str),(str,'head'->str)],boolean,str,str,str,Struct[(str,'login'->str),(str,'type'->str),(str,'gravatar_id'->str),(str,'email'->str)],Struct[(str,'id'->i64),(str,'name'->str),(str,'url'->str),(str,'description'->str),(str,'watchers'->i64),(str,'stargazers'->i64),(str,'forks'->i64),(str,'fork'->boolean),(str,'size'->i64),(str,'owner'->str),(str,'private'->boolean),(str,'open_issues'->i64),(str,'has_issues'->boolean),(str,'has_downloads'->boolean),(str,'has_wiki'->boolean),(str,'language'->str),(str,'created_at'->str),(str,'pushed_at'->str),(str,'master_branch'->str)],i64,i64)) -> (str,i64,i64)\n[2022-11-08 05:49:49.560] [codegen] [warning] hack, need to fix stuff here...\n[2022-11-08 05:49:49.568] [hyper specializer] [info] generated code in 0.021756s\n[2022-11-08 05:49:49.636] [Lambda worker] [info] specialized normal-case type (str,Struct[(str,'shas'->List[(str,str,str,str,boolean)]),(str,'size'->i64),(str,'ref'->str),(str,'head'->str)],boolean,str,str,str,Struct[(str,'login'->str),(str,'type'->str),(str,'gravatar_id'->str),(str,'email'->str)],Struct[(str,'id'->i64),(str,'name'->str),(str,'url'->str),(str,'description'->str),(str,'watchers'->i64),(str,'stargazers'->i64),(str,'forks'->i64),(str,'fork'->boolean),(str,'size'->i64),(str,'owner'->str),(str,'private'->boolean),(str,'open_issues'->i64),(str,'has_issues'->boolean),(str,'has_downloads'->boolean),(str,'has_wiki'->boolean),(str,'language'->str),(str,'created_at'->str),(str,'pushed_at'->str),(str,'master_branch'->str)]) is different than given normal-case type (str,boolean,Struct[(str,'avatar_url'->str),(str,'gravatar_id'=>str),(str,'id'=>i64),(str,'login'=>str),(str,'url'->str)],str,Struct[(str,'action'=>str),(str,'comment'=>Struct[(str,'body'->str),(str,'commit_id'=>str),(str,'created_at'->str),(str,'id'->i64),(str,'line'=>null),(str,'path'=>null),(str,'position'=>null),(str,'updated_at'->str),(str,'url'->str),(str,'user'->Struct[(str,'avatar_url'->str),(str,'gravatar_id'->str),(str,'id'->i64),(str,'login'->str),(str,'url'->str)])]),(str,'commits'=>List[Struct[(str,'sha'->str),(str,'author'->Struct[(str,'name'->str),(str,'email'->str)]),(str,'url'->str),(str,'message'->str)]]),(str,'description'=>str),(str,'forkee'=>Struct[(str,'clone_url'->str),(str,'created_at'->str),(str,'description'->str),(str,'fork'->boolean),(str,'forks'->i64),(str,'git_url'->str),(str,'homepage'->str),(str,'html_url'->str),(str,'id'->i64),(str,'language'->Option[str]),(str,'master_branch'->null),(str,'name'->str),(str,'open_issues'->i64),(str,'owner'->Struct[(str,'avatar_url'->str),(str,'gravatar_id'->str),(str,'url'->str),(str,'id'->i64),(str,'login'->str)]),(str,'private'->boolean),(str,'public'->boolean),(str,'pushed_at'->str),(str,'size'->i64),(str,'ssh_url'->str),(str,'svn_url'->str),(str,'updated_at'->str),(str,'url'->str),(str,'watchers'->i64)]),(str,'gist'=>Struct[(str,'comments'->i64),(str,'created_at'->str),(str,'description'->str),(str,'files'->{}),(str,'git_pull_url'->str),(str,'git_push_url'->str),(str,'html_url'->str),(str,'id'->str),(str,'public'->boolean),(str,'updated_at'->str),(str,'url'->str),(str,'user'->Option[Struct[(str,'avatar_url'->str),(str,'gravatar_id'->str),(str,'url'->str),(str,'id'->i64),(str,'login'->str)]])]),(str,'head'=>str),(str,'issue'=>Struct[(str,'assignee'->Option[Struct[(str,'gravatar_id'->str),(str,'avatar_url'->str),(str,'url'->str),(str,'id'->i64),(str,'login'->str)]]),(str,'body'->str),(str,'closed_at'->Option[str]),(str,'comments'->i64),(str,'created_at'->str),(str,'html_url'->str),(str,'id'->i64),(str,'labels'->List[Struct[(str,'name'->str),(str,'url'->str),(str,'color'->str)]]),(str,'milestone'->Option[Struct[(str,'number'->i64),(str,'created_at'->str),(str,'due_on'->str),(str,'title'->str),(str,'creator'->Struct[(str,'gravatar_id'->str),(str,'avatar_url'->str),(str,'url'->str),(str,'id'->i64),(str,'login'->str)]),(str,'url'->str),(str,'open_issues'->i64),(str,'closed_issues'->i64),(str,'description'->str),(str,'state'->str)]]),(str,'number'->i64),(str,'pull_request'->Struct[(str,'diff_url'->Option[str]),(str,'html_url'->Option[str]),(str,'patch_url'->Option[str])]),(str,'state'->str),(str,'title'->str),(str,'updated_at'->str),(str,'url'->str),(str,'user'->Struct[(str,'avatar_url'->str),(str,'gravatar_id'->str),(str,'id'->i64),(str,'login'->str),(str,'url'->str)])]),(str,'legacy'=>Struct[(str,'action'=>str),(str,'comment_id'=>i64),(str,'commit'=>str),(str,'desc'=>str),(str,'head'=>str),(str,'id'=>i64),(str,'issue'=>i64),(str,'issue_id'=>i64),(str,'name'=>str),(str,'number'=>i64),(str,'push_id'=>i64),(str,'ref'=>str),(str,'shas'=>List[List[str]]),(str,'size'=>i64),(str,'url'=>str)]),(str,'master_branch'=>str),(str,'pages'=>List[Struct[(str,'sha'->str),(str,'title'->str),(str,'action'->str),(str,'page_name'->str),(str,'summary'->null)]]),(str,'push_id'=>i64),(str,'ref'=>Option[str]),(str,'ref_type'=>str),(str,'size'=>i64)],str,Struct[(str,'id'=>i64),(str,'name'->str),(str,'url'->str)],Option[Struct[(str,'gravatar_id'->str),(str,'url'->str),(str,'avatar_url'->str),(str,'id'->i64),(str,'login'->str)]]).\n[2022-11-08 05:49:49.636] [Lambda worker] [info] -- hyperspecialization took 1.228372s\n[2022-11-08 05:49:49.667] [global] [info] TransformStage - Optimization via LLVM passes took 0.030762 ms\n[2022-11-08 05:49:49.667] [global] [info] syms registered (or skipped), compile now\n[2022-11-08 05:49:49.704] [global] [info] functor fast_Stage_0 retrieved from llvm\n[2022-11-08 05:49:49.704] [global] [info] Compiled code paths for stage 0 in 0.07 ms\n[2022-11-08 05:49:49.704] [Lambda worker] [info] calling initStageFunctor with 0 args\n[2022-11-08 05:49:56.808] [Lambda worker] [info] fast path took: 7.103175s\n[2022-11-08 05:49:56.808] [Lambda worker] [info] Input rows processed: normal: 61470 unresolved: 87413\n[2022-11-08 05:49:56.808] [Lambda worker] [info] Normal rows before resolve: 0, except rows before resolve: 87413\n[2022-11-08 05:49:56.808] [Lambda worker] [info] Starting exception resolution/slow path execution\n[2022-11-08 05:49:56.808] [Lambda worker] [info] has interpreter path: yes has compiled fallback path: yes\n[2022-11-08 05:49:56.808] [Lambda worker] [info] compiling slow code path b.c. exceptions occurred.\n[2022-11-08 05:49:58.227] [Lambda worker] [info] Compilation of slow path took 1.419162s\n[2022-11-08 05:49:58.288] [Lambda worker] [info] first unresolved row ec code result is: ec code: 112\n[2022-11-08 05:50:03.397] [Lambda worker] [info] Resolved buffer, compiled: 0 interpreted: 41727 unresolved: 3005\n[2022-11-08 05:50:03.463] [Lambda worker] [info] first unresolved row ec code result is: ec code: 112\n[2022-11-08 05:50:08.479] [Lambda worker] [info] Resolved buffer, compiled: 0 interpreted: 39753 unresolved: 2928\n[2022-11-08 05:50:08.483] [Lambda worker] [info] Exception resolution/slow path done. Took 11.675077s\n[2022-11-08 05:50:08.483] [Lambda worker] [info] Data processed in 18.7784s 3.79 MB (5444 normal rows) 2.73 MB (5933 exception rows)  128.03 KB (0 hash rows)\n[2022-11-08 05:50:08.483] [Lambda worker] [info] Writing data to s3://tuplex-leonhard/experiments/flights_github/hyper/part1.csv.csv\n[2022-11-08 05:50:08.483] [Lambda worker] [info] file output initiated...\n[2022-11-08 05:50:08.483] [Lambda worker] [info] Writing output from 2 parts (3.79 MB, 5444 rows)\n[2022-11-08 05:50:08.483] [Lambda worker] [info] Merging output parts together into s3://tuplex-leonhard/experiments/flights_github/hyper/part1.csv.csv\n[2022-11-08 05:50:08.483] [s3fs] [info] Writing buffer of size 18\n[2022-11-08 05:50:08.483] [s3fs] [info] Writing buffer of size 2186784\n[2022-11-08 05:50:08.484] [s3fs] [info] Writing buffer of size 1785740\n[2022-11-08 05:50:08.485] [s3fs] [info] Invoking lazyUpload to uri s3://tuplex-leonhard/experiments/flights_github/hyper/part1.csv.csv\n[2022-11-08 05:50:08.485] [s3fs] [info] Issuing simple write request\n[2022-11-08 05:50:08.682] [Lambda worker] [info] file output done.\n[2022-11-08 05:50:08.682] [Lambda worker] [info] Data fully materialized\n[2022-11-08 05:50:08.682] [Lambda worker] [info] Took 18.977610s in total\n[2022-11-08 05:50:08.682] [Lambda worker] [info] Paths rows took: normal: 61470 general: 0 interpreter: 81480 unresolved: 5933\n","invoked_requests":[],"input_uris":["s3://tuplex-public/data/github_daily/2013-10-15.json:0-268435456"]}],"requests":[{"requestId":"7e104a68-7fde-4dde-b740-3ce3c88e8145","containerId":"26700c8c-c0db-4648-9e9d-d1e518e0ca84","durationInMs":9155.49,"billedDurationInMs":9156,"memorySizeInMb":10000,"maxMemoryUsedInMb":4525,"returnCode":0,"errorMessage":"","tsRequestStart":1667886615229756531,"tsRequestEnd":1667886624482125162},{"requestId":"ef326473-8f92-4979-9e97-6177434ea0ed","containerId":"d9c04bb4-f3f8-43d3-92ec-af2fd7adc394","durationInMs":20311.1,"billedDurationInMs":20312,"memorySizeInMb":10000,"maxMemoryUsedInMb":4525,"returnCode":0,"errorMessage":"","tsRequestStart":1667886615234209990,"tsRequestEnd":1667886635693790017}]}